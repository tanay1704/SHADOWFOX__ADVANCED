{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1284c4d",
   "metadata": {},
   "source": [
    "\n",
    "# Advanced NLP/ML Project: Language Model Analysis\n",
    "\n",
    "## Problem Statement\n",
    "Embark on an AI-driven journey in the realm of natural language processing (NLP) and machine learning (ML) by deploying a Language Model (LM) of your choice. This project involves implementing the chosen LM, analyzing its performance, and visualizing its capabilities in a Jupyter Notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c0d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required packages\n",
    "!pip install transformers datasets matplotlib seaborn --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbb4a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BertTokenizer, BertModel\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab6ab9",
   "metadata": {},
   "source": [
    "## Load and Use GPT-2 for Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aec721",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "prompt = \"Artificial Intelligence is transforming\"\n",
    "generated = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Generated Text:\", generated[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c3d1d",
   "metadata": {},
   "source": [
    "## Use BERT for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49a5621",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "sample_text = \"I love how AI is changing the future!\"\n",
    "result = classifier(sample_text)\n",
    "print(\"Input:\", sample_text)\n",
    "print(\"Sentiment Result:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b0a401",
   "metadata": {},
   "source": [
    "## Visualize Attention Weights from BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c9b8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load BERT model with attention outputs\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\", output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"AI is powerful.\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "attentions = outputs.attentions  # List of attention layers\n",
    "\n",
    "# Visualize one attention head of the last layer\n",
    "attention = attentions[-1][0, 0].detach().numpy()\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens, cmap=\"viridis\")\n",
    "plt.title(\"BERT Attention Heatmap (Last Layer, Head 0)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed2de77",
   "metadata": {},
   "source": [
    "\n",
    "## Research Questions\n",
    "1. How effectively does the LM retain context in long text?\n",
    "2. Can the LM generate creative and original content?\n",
    "3. How well does it adapt to legal/medical/technical inputs?\n",
    "4. What are the limitations of the LM in terms of bias or factual errors?\n",
    "5. How does its performance compare to traditional models?\n",
    "\n",
    "## Research Objectives\n",
    "- Evaluate contextual retention\n",
    "- Assess creativity and diversity\n",
    "- Test domain adaptability\n",
    "- Analyze ethical limitations and biases\n",
    "- Compare performance across tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32018e2",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion & Insights\n",
    "\n",
    "- **GPT-2** performs well in creative text generation but may hallucinate facts.\n",
    "- **BERT** is powerful for classification and interpretability, especially with attention maps.\n",
    "- **Attention visualization** gives insights into how the model weighs input tokens.\n",
    "\n",
    "### Insights\n",
    "- Domain-specific models (BioBERT, LegalBERT) may improve accuracy in niche fields.\n",
    "- Ethical considerations like hallucinations and bias must be addressed.\n",
    "- Visual tools help build trust and transparency in LM behavior.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
